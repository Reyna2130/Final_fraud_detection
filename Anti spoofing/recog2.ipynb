{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7c729-0093-4859-9827-8c47aca6029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "size = 4\n",
    "haar_face_cascade = 'haarcascade_frontalface_default.xml'\n",
    "datasets = r'C:\\Users\\aanki\\Anti spooling\\datasets'  # Adjust this path as needed\n",
    "\n",
    "# Load Dlib's pre-trained shape predictor model\n",
    "shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "# Function to calculate eye aspect ratio (EAR)\n",
    "def eye_aspect_ratio(eye_points):\n",
    "    A = np.linalg.norm(eye_points[1] - eye_points[5])\n",
    "    B = np.linalg.norm(eye_points[2] - eye_points[4])\n",
    "    C = np.linalg.norm(eye_points[0] - eye_points[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Part 1: Create LBPHFaceRecognizer\n",
    "print('Recognizing Face. Please be in sufficient light...')\n",
    "\n",
    "# Create a list of images and a list of corresponding names\n",
    "(images, labels, names, id) = ([], [], {}, 0)\n",
    "for (subdirs, dirs, files) in os.walk(datasets):\n",
    "    for subdir in dirs:\n",
    "        names[id] = subdir\n",
    "        subjectpath = os.path.join(datasets, subdir)\n",
    "        for filename in os.listdir(subjectpath):\n",
    "            path = os.path.join(subjectpath, filename)\n",
    "            img = cv2.imread(path, 0)\n",
    "            if img is not None:  # Ensure the image was loaded\n",
    "                img = cv2.resize(img, (130, 100))  # Resize images\n",
    "                images.append(img)\n",
    "                labels.append(id)\n",
    "        id += 1\n",
    "\n",
    "(width, height) = (130, 100)\n",
    "\n",
    "# Create a Numpy array from the two lists above\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# OpenCV trains a model from the images\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.train(images, labels)\n",
    "\n",
    "# Part 2: Use LBPHFaceRecognizer on camera stream\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + haar_face_cascade)\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters for motion detection and blink detection\n",
    "ret, prev_frame = webcam.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "motion_threshold = 10000  # Adjust this threshold based on your testing\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "blink_threshold = 5  # Adjust based on typical blink behavior\n",
    "confidence_threshold = 70  # Threshold for considering a face as recognized\n",
    "required_blinks = 3  # Number of blinks required to declare as real\n",
    "\n",
    "blink_counts = {}  # Dictionary to keep track of blinks for each detected face\n",
    "\n",
    "while True:\n",
    "    ret, frame = webcam.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Check for motion\n",
    "    diff_frame = cv2.absdiff(prev_gray, gray)\n",
    "    _, thresh_frame = cv2.threshold(diff_frame, 25, 255, cv2.THRESH_BINARY)\n",
    "    motion = np.sum(thresh_frame)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        prediction = model.predict(face_resize)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "\n",
    "        if prediction[1] < confidence_threshold:\n",
    "            name = names[prediction[0]]\n",
    "            status = \"Registered\"\n",
    "            color = (0, 255, 0)  # Green for registered\n",
    "        else:\n",
    "            name = \"Not Registered\"\n",
    "            status = \"Not Registered\"\n",
    "            color = (0, 0, 255)  # Red for not registered\n",
    "\n",
    "        cv2.putText(frame, f'{name}', (x - 10, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color)\n",
    "        cv2.putText(frame, status, (x - 10, y + h + 10), cv2.FONT_HERSHEY_PLAIN, 1, color)\n",
    "\n",
    "        # Convert the face region back to a format dlib can work with\n",
    "        dlib_rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "        shape = predictor(gray, dlib_rect)\n",
    "        shape = np.array([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "        left_eye = shape[36:42]\n",
    "        right_eye = shape[42:48]\n",
    "\n",
    "        ear_left = eye_aspect_ratio(left_eye)\n",
    "        ear_right = eye_aspect_ratio(right_eye)\n",
    "\n",
    "        if (ear_left + ear_right) / 2.0 < 0.25:\n",
    "            blink_counter += 1\n",
    "            if blink_counter >= blink_threshold:\n",
    "                total_blinks += 1\n",
    "                blink_counter = 0\n",
    "\n",
    "                if prediction[0] in blink_counts:\n",
    "                    blink_counts[prediction[0]] += 1\n",
    "                else:\n",
    "                    blink_counts[prediction[0]] = 1\n",
    "\n",
    "        # Liveness detection\n",
    "        if name != \"Not Registered\" and blink_counts.get(prediction[0], 0) >= required_blinks:\n",
    "            liveness_status = \"Real\"\n",
    "            liveness_color = (0, 255, 0)  # Green for real\n",
    "        elif motion < motion_threshold:\n",
    "            liveness_status = \"Fake\"\n",
    "            liveness_color = (0, 0, 255)  # Red for fake\n",
    "        else:\n",
    "            liveness_status = \"Undetermined\"\n",
    "            liveness_color = (0, 255, 255)  # Yellow for undetermined\n",
    "\n",
    "        cv2.putText(frame, liveness_status, (x - 10, y + h + 20), cv2.FONT_HERSHEY_PLAIN, 1, liveness_color)\n",
    "\n",
    "    cv2.imshow('OpenCV', frame)\n",
    "    prev_gray = gray\n",
    "\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c0bcbb-d528-4ab7-a7a2-309e69eda8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing Face. Please be in sufficient light...\n",
      "Can't Identify\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import os\n",
    "\n",
    "size = 4\n",
    "haar_face_cascade = 'haarcascade_frontalface_default.xml'\n",
    "datasets = r'C:\\Users\\aanki\\Anti spooling\\datasets'  # Adjust this path as needed\n",
    "\n",
    "# Load Dlib's pre-trained shape predictor model\n",
    "shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "# Function to calculate eye aspect ratio (EAR)\n",
    "def eye_aspect_ratio(eye_points):\n",
    "    A = np.linalg.norm(eye_points[1] - eye_points[5])\n",
    "    B = np.linalg.norm(eye_points[2] - eye_points[4])\n",
    "    C = np.linalg.norm(eye_points[0] - eye_points[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Part 1: Create LBPHFaceRecognizer\n",
    "print('Recognizing Face. Please be in sufficient light...')\n",
    "\n",
    "# Create a list of images and a list of corresponding names\n",
    "(images, labels, names, id) = ([], [], {}, 0)\n",
    "for (subdirs, dirs, files) in os.walk(datasets):\n",
    "    for subdir in dirs:\n",
    "        names[id] = subdir\n",
    "        subjectpath = os.path.join(datasets, subdir)\n",
    "        for filename in os.listdir(subjectpath):\n",
    "            path = os.path.join(subjectpath, filename)\n",
    "            img = cv2.imread(path, 0)\n",
    "            if img is not None:  # Ensure the image was loaded\n",
    "                img = cv2.resize(img, (130, 100))  # Resize images\n",
    "                images.append(img)\n",
    "                labels.append(id)\n",
    "        id += 1\n",
    "\n",
    "(width, height) = (130, 100)\n",
    "\n",
    "# Create a Numpy array from the two lists above\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# OpenCV trains a model from the images\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.train(images, labels)\n",
    "\n",
    "# Part 2: Use LBPHFaceRecognizer on camera stream\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + haar_face_cascade)\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "# Parameters for motion detection and blink detection\n",
    "ret, prev_frame = webcam.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "motion_threshold = 10000  # Adjust this threshold based on your testing\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "blink_threshold = 5  # Adjust based on typical blink behavior\n",
    "confidence_threshold = 70  # Threshold for considering a face as recognized\n",
    "required_blinks = 3  # Number of blinks required to declare as real\n",
    "\n",
    "blink_counts = {}  # Dictionary to keep track of blinks for each detected face\n",
    "real_face_count = 0\n",
    "fake_or_undetermined_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = webcam.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Check for motion\n",
    "    diff_frame = cv2.absdiff(prev_gray, gray)\n",
    "    _, thresh_frame = cv2.threshold(diff_frame, 25, 255, cv2.THRESH_BINARY)\n",
    "    motion = np.sum(thresh_frame)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face_resize = cv2.resize(face, (width, height))\n",
    "        prediction = model.predict(face_resize)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "\n",
    "        if prediction[1] < confidence_threshold:\n",
    "            name = names[prediction[0]]\n",
    "            status = \"Registered\"\n",
    "            color = (0, 255, 0)  # Green for registered\n",
    "        else:\n",
    "            name = \"Not Registered\"\n",
    "            status = \"Not Registered\"\n",
    "            color = (0, 0, 255)  # Red for not registered\n",
    "\n",
    "        cv2.putText(frame, f'{name}', (x - 10, y - 10), cv2.FONT_HERSHEY_PLAIN, 1, color)\n",
    "        cv2.putText(frame, status, (x - 10, y + h + 10), cv2.FONT_HERSHEY_PLAIN, 1, color)\n",
    "\n",
    "        # Convert the face region back to a format dlib can work with\n",
    "        dlib_rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "        shape = predictor(gray, dlib_rect)\n",
    "        shape = np.array([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "        left_eye = shape[36:42]\n",
    "        right_eye = shape[42:48]\n",
    "\n",
    "        ear_left = eye_aspect_ratio(left_eye)\n",
    "        ear_right = eye_aspect_ratio(right_eye)\n",
    "\n",
    "        if (ear_left + ear_right) / 2.0 < 0.25:\n",
    "            blink_counter += 1\n",
    "            if blink_counter >= blink_threshold:\n",
    "                total_blinks += 1\n",
    "                blink_counter = 0\n",
    "\n",
    "                if prediction[0] in blink_counts:\n",
    "                    blink_counts[prediction[0]] += 1\n",
    "                else:\n",
    "                    blink_counts[prediction[0]] = 1\n",
    "\n",
    "        # Liveness detection\n",
    "        if name != \"Not Registered\" and blink_counts.get(prediction[0], 0) >= required_blinks:\n",
    "            liveness_status = \"Real\"\n",
    "            liveness_color = (0, 255, 0)  # Green for real\n",
    "            real_face_count += 1\n",
    "        elif motion < motion_threshold:\n",
    "            liveness_status = \"Fake\"\n",
    "            liveness_color = (0, 0, 255)  # Red for fake\n",
    "            fake_or_undetermined_count += 1\n",
    "        else:\n",
    "            liveness_status = \"Undetermined\"\n",
    "            liveness_color = (0, 255, 255)  # Yellow for undetermined\n",
    "            fake_or_undetermined_count += 1\n",
    "\n",
    "        cv2.putText(frame, liveness_status, (x - 10, y + h + 20), cv2.FONT_HERSHEY_PLAIN, 1, liveness_color)\n",
    "\n",
    "    cv2.imshow('OpenCV', frame)\n",
    "    prev_gray = gray\n",
    "\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "    if real_face_count >= 100:\n",
    "        print(\"Live Detected\")\n",
    "        break\n",
    "    elif fake_or_undetermined_count >= 100:\n",
    "        print(\"Can't Identify\")\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790d87c-fab3-4523-bff5-1fdebf03a8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07851081-6af2-4028-8f85-1e549df21c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
